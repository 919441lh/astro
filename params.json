{"name":"Astro","tagline":"High performance SQL layer over HBase by using Spark SQL framework","body":"## Astro: Fast SQL on HBase using SparkSQL\r\n\r\nAstro is fully distributed SQL engine on HBase by leveraging Spark ecosystem. It enables systematic and powerful handling of data pruning, intelligent scan, and pushdowns like custom filters and coprocessor, and make more traditional RDBS capabilities possible.\r\n\r\n## Requirements\r\n\r\nThis version of 1.0.0 requires Spark 1.4.0,HBase 0.98\r\n\r\n## Building Spark HBase\r\n\r\nSpark HBase is built using [Apache Maven](http://maven.apache.org/).\r\n\r\nI. Clone and build HuaweiBigData/astro\r\n```\r\n$ git clone https://github.com/HuaweiBigData/astro\r\n```\r\n\r\nII. Go to the root of the source tree\r\n```\r\n$ cd astro\r\n```\r\n\r\nIII. Build the project\r\nBuild without testing\r\n```\r\n$ mvn -DskipTests clean install \r\n```\r\nOr, build with testing. It will run test suites against a HBase minicluster.\r\n```\r\n$ mvn clean install\r\n```\r\n\r\n## Activate Coprocessor and Custom Filter in HBase\r\n\r\nFirst, add the path of spark-hbase jar to the hbase-env.sh in $HBASE_HOME/conf directory, as follows:\r\n```\r\nHBASE_CLASSPATH=$HBASE_CLASSPATH:/spark-hbase-root-dir/target/spark-sql-on-hbase-1.0.0.jar\r\n```\r\nThen, register the coprocessor service 'CheckDirEndPoint' to hbase-site.xml in the same directory, as follows:\r\n```\r\n<property>\r\n    <name>hbase.coprocessor.region.classes</name>\r\n    <value>org.apache.spark.sql.hbase.CheckDirEndPointImpl</value>\r\n</property>\r\n```\r\n(Warning: Don't register another coprocessor service 'SparkSqlRegionObserver' here !)\r\n\r\n\r\n## Interactive Scala Shell\r\n\r\nThe easiest way to start using Spark HBase is through the Scala shell:\r\n```\r\n./bin/hbase-sql\r\n```\r\n\r\n## Python Shell\r\n\r\nFirst, add the spark-hbase jar to the SPARK_CLASSPATH in the $SPARK_HOME/conf directory, as follows:\r\n```\r\nSPARK_CLASSPATH=$SPARK_CLASSPATH:/spark-hbase-root-dir/target/spark-sql-on-hbase-1.0.0.jar\r\n```\r\nThen go to the spark-hbase installation directory and issue\r\n```\r\n./bin/pyspark-hbase\r\n```\r\nA successfull message is as follows:\r\n\r\n   You are using Spark SQL on HBase!!!\r\n   HBaseSQLContext available as hsqlContext.\r\n\r\nTo run a python script, the PYTHONPATH environment should be set to the \"python\" directory of the Spark-HBase installation. For example,\r\n```\r\nexport PYTHONPATH=/root-of-Spark-HBase/python\r\n```\r\n\r\nNote that the shell commands are not included in the Zip file of the Spark release. They are for developers' use only for this version of 1.0.0. Instead, users can use \"$SPARK_HOME/bin/spark-shell --packages Huawei-Spark/Spark-SQL-on-HBase:1.0.0\" for SQL shell or \"$SPARK_HOME/bin/pyspark --packages Huawei-Spark/Spark-SQL-on-HBase:1.0.0\" for Pythin shell.\r\n\r\n## Running Tests\r\n\r\nTesting first requires [building Spark HBase](#building-spark-hbase). Once Spark HBase is built ...\r\n\r\nRun all test suites from Maven:\r\n```\r\nmvn -Phbase,hadoop-2.4 test\r\n```\r\nRun a single test suite from Maven, for example:\r\n```\r\nmvn -Phbase,hadoop-2.4 test -DwildcardSuites=org.apache.spark.sql.hbase.BasicQueriesSuite\r\n```\r\n## IDE Setup\r\n\r\nWe use IntelliJ IDEA for Spark HBase development. You can get the community edition for free and install the JetBrains Scala plugin from Preferences > Plugins.\r\n\r\nTo import the current Spark HBase project for IntelliJ:\r\n\r\n1. Download IntelliJ and install the Scala plug-in for IntelliJ. You may also need to install Maven plug-in for IntelliJ.\r\n2. Go to \"File -> Import Project\", locate the Spark HBase source directory, and select \"Maven Project\".\r\n3. In the Import Wizard, select \"Import Maven projects automatically\" and leave other settings at their default. \r\n4. Make sure some specific profiles are enabled. Select corresponding Hadoop version, \"maven3\" and also\"hbase\" in order to get dependencies.\r\n5. Leave other settings at their default and you should be able to start your development.\r\n6. When you run the scala test, sometimes you will get out of memory exception. You can increase your VM memory usage by the following setting, for example:\r\n\r\n```\r\n-XX:MaxPermSize=512m -Xmx3072m\r\n```\r\n\r\nYou can also make those setting to be the default by setting to the \"Defaults -> ScalaTest\".\r\n\r\n## Configuration\r\n\r\nPlease refer to the [Configuration guide](http://spark.apache.org/docs/latest/configuration.html)\r\nin the online documentation for an overview on how to configure Spark.\r\n\r\n## Fork and Contribute\r\nThis is an 100% open source project, we are always open to people who want to use the system or contribute to it. \r\nThis guide document introduce [how to contribute to Astro](https://github.com/HuaweiBigData/astro/wiki/How-to-contribute-to-Astro).\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}