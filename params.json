{"name":"Astro","tagline":"High performance SQL layer over HBase by using Spark SQL framework","body":"## Astro: Fast SQL on HBase using SparkSQL\r\n\r\nAstro is fully distributed SQL engine on HBase by leveraging Spark ecosystem. It enables systematic and powerful handling of data pruning, intelligent scan, and pushdowns like custom filters and coprocessor, and make more traditional RDBMS capabilities possible.\r\n![ASTRO](http://ravipesala.github.io/astro/images/astro_architecture.png)\r\n\r\n## Why Astro\r\n\r\nHBase is a very useful big data store but its access mechanism is very primitive and only through client-side APIs, Map/Reduce interfaces and interactive shells. Astro provides SQL layer over Hbase, so it takes your SQL query, compiles it into optimized Spark plan and in turn it does series of HBase scans to result sets.User do not require to write any complex client code to access HBase tables.\r\nFor example,to count rows in Hbase table we need to write following code:\r\n```\r\npublic class MyAggregationClient {\r\n    private static final byte[] TABLE_NAME = Bytes.toBytes(\"CNCC_Demo\");\r\n    private static final byte[] CF = Bytes.toBytes(\"f1\");\r\n    public static void main(String[] args) throws Throwable \r\n    {\r\n        Configuration customConf = new Configuration();\r\n        customConf.setStrings(\"hbase.zookeeper.quorum\",\"node0,node1,node2\");\r\n        customConf.setLong(\"hbase.rpc.timeout\", 600000);\r\n        customConf.setLong(\"hbase.client.scanner.caching\", 1000);\r\n        Configuration configuration = HBaseConfiguration.create(customConf);\r\n        AggregationClient aggregationClient = new AggregationClient(configuration);\r\n        Scan scan = new Scan();        \r\n        scan.addFamily(CF);\r\n        long rowCount = aggregationClient.rowCount(TABLE_NAME, null, scan);\r\n        System.out.println(\"row count is \" + rowCount);    \r\n     }\r\n}\r\n```\r\nBut if we use Astro,create table schema and just one SQL query:\r\n```\r\nCREATE TABLE CNCC_Demo (\r\n      index          INTEGER,\r\n      event_time     STRING,\r\n      country        STRING,\r\n      site           STRING,\r\n      PRIMARY KEY(index))\r\n      MAPPED BY (CNCC_Demo, COLS=   [event_time=f1.c1,country=f1.c2,site=f2.c1]);\r\nselect  count(*)  from CNCC_Demo;\r\n```\r\n\r\n## Quick Start\r\n\r\nThe easiest way to start using Astro is through the shell:\r\n\r\n### Interactive Scala Shell\r\n\r\nIn this shell Spark context and as well as HBaseSQLContext is already created, launch the Astro shell as per the following method, and in this shell should be SQL apart from HELP and EXIT.\r\n```\r\n>./bin/hbase-sql\r\nWelcome to hbaseql CLI\r\n\r\nastro>show tables;\r\nOK\r\n+---------+-----------+\r\n|tableName|isTemporary|\r\n+---------------------+\r\n|Employee |      false|\r\n+---------------------+\r\n\r\nTime taken : 1.231 seconds\r\nastro>help;\r\nUsage: HELP Statement\r\n      Statement:\r\n         CREATE | DROP | ALTER | LOAD | SELECT | INSERT | DESCRIBE | SHOW\r\n```\r\n\r\n### Python Shell\r\n\r\nFirst, add the spark-hbase jar to the SPARK_CLASSPATH in the $SPARK_HOME/conf directory, as follows:\r\n```\r\nSPARK_CLASSPATH=$SPARK_CLASSPATH:/spark-hbase-root-dir/target/spark-sql-on-hbase-1.0.0.jar\r\n```\r\nThen go to the Astro installation directory and launch the Astro shell:\r\n```\r\n./bin/pyspark-hbase\r\n```\r\nA successful message is as follows:\r\nYou are using Spark SQL on HBase! HBaseSQLContext is available as hsqlContext.\r\n\r\nTo run a python script, the PYTHONPATH environment should be set to the \"python\" directory of the Spark-HBase installation. For example,\r\n```\r\nexport PYTHONPATH=/root-of-Spark-HBase/python\r\n```\r\n\r\n## SQL Support\r\n\r\nQueries and data types will be the same as what SparkSQL supports. The differences will be in DDL and DML.\r\n\r\n### DDL\r\nNote that all DDL statements only affect the logical SQL table and not the physical tables.\r\n\r\n### CREATE TABLE\r\nA create table statement will be of the form of:\r\n```\r\nCREATE TABLE table_name (col1 TYPE1, col2 TYPE2, …, PRIMARY KEY (col7, col1, col3)) \r\nMAPPED BY (hbase_tablename, COLS=[col2=cf1.cq11, col4=cf1.cq12, col5=cf2.cq21, col6=cf2.cq22])\r\n```\r\nA SQL table on HBASE is a basically logical table mapped to a HBase table,this mapping can be many-to-one to support “schema-on-read” for SQL access to HBase data.\r\n\r\n* “hbase_table_name” denotes the HBase table\r\n* “primary key” constraint denotes the HBase row key composition of columns\r\n* “col2=cf1.cq1” denotes the mapping of the second column to the HBase tables column qualifier of “cq1” of column family “cf1”. Note : The table and the column families specified have to be exist in HBase for the CREATE TABLE statement to succeed. In addition, the columns in the primary key cannot be mapped to another column family/column qualifier combo. Other normal SQL sanity checks, such as uniqueness of logical columns, will be applied as well.\r\n* Row Key Composition :\r\nHBase row keys will be composed in the way of big endian for processing efficiency. Keys or key components of the STRING type are marked with a NULL terminator.\r\n\r\n### DROP TABLE\r\nA drop table statement is of the form of\r\n```\r\nDROP TABLE table_name;\r\n```\r\nThis will not delete the HBase table the SQL table maps to, but just deletes the SQL table with its schema.\r\n\r\n### ALTER TABLE\r\n```\r\nALTER TABLE table_name DROP column;\r\n```\r\n\r\nDrops an existing column from the SQL table.\r\n\r\n```\r\nALTER TABLE table_name ADD col1 TYPE1 MAPPED BY (col1 = cf.cq)\r\n```\r\nAdds a new column that is mapped to existing column family “cf” and column qualifier “cq”,\r\nALTER TABLE does not support addition or deletion of components in the composite row key\r\n\r\n## DML\r\n\r\n### INSERT\r\nThe syntax remains the same as SchemaRDD’s. One constraint is that all columns in the HBASE key must be present for insertion to succeed. Normal SQL sanity checks for INSERT, such as uniqueness of logical columns, will be applied. There are two types of inserts. The first has the following syntax:\r\n\r\n```\r\nINSERT INTO TABLE table_name  VALUES (col1_value, col2_value, …);\r\n```\r\n\r\nWhile the second has\r\n\r\n```\r\nINSERT INTO TABLE table1_name SELECT … FROM table2_name;\r\n```\r\n\r\n### Bulk Loading\r\n```\r\nLOAD DATA [PARALLEL] INPATH filePath [OVERWRITE] INTO TABLE tableName [FIELDS TERMINATED BY char]\r\n```\r\nIt is similar to Hive command, it is used to invoke a bulk loading into existing HBase tables. A “parallel” option will merge the “incremental loading” phase into the HFile generation phase. Conceivably it will perform better, particularly for non-presplit tables.\r\n\r\n### Data Frame\r\nData Frame’s functionalities are supported in Astro. An example statement is as follows:\r\n```\r\nval  hbaseContext =  new HBaseSQLContext(sc)\r\nhbaseContext.read.format(“org.apache.spark.sql.hbase.HBaseSource”).options(\r\n    Map(\"namespace\" -> \"\", \"tableName\" -> \"people\", \"hbaseTableName\" -> \"people_table\",\r\n      \"colsSeq\" -> \"name,age,id,address\",\r\n      \"keyCols\" -> \"id,integer\",\r\n      \"nonKeyCols\" ->   \r\n        \"name,string,cf1,cq_name;age,integer,cf1,cq_age;address,string,cf2,cq_address\")).load\r\nhbaseContext.sql(\"Select  `personal_data:name`, `personal_data:identification` as b, `personal_data\r\nhbaseContext.sql(\"Select  name,  id, address  from people\").collect.foreach(println)\r\n```\r\nThere will be a few potentially subtle difference worth of caution. In particular, the methods of “registerAsTable” will not “register” a HBase-based table but an usual SparkSQL table. On the other hand, “insertInto” and “saveAsTable method will insert data into an existing SQL table on top of a HBase table.\r\n\r\n### Metadata Persistence\r\nTable meta data is stored in a HBase table named “SPARK_SQL_HBASE_TABLE”, of a single column family named “CF”. Each SQL table uses a single row in the HBase table. And each column will store the name and type encoding of a column of the SQL table.\r\n\r\n## Fork and Contribute\r\nThis is an 100% open source project, we are always open to people who want to use the system or contribute to it. \r\nThis guide document introduce [how to contribute to Astro](https://github.com/HuaweiBigData/astro/wiki/How-to-contribute-to-Astro).","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}