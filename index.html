<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Astro by HuaweiBigData</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Astro</h1>
      <h2 class="project-tagline">High performance SQL layer over HBase by using Spark SQL framework</h2>
      <a href="https://github.com/HuaweiBigData/astro" class="btn">View on GitHub</a>
      <a href="https://github.com/HuaweiBigData/astro/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/HuaweiBigData/astro/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h2>
<a id="astro-fast-sql-on-hbase-using-sparksql" class="anchor" href="#astro-fast-sql-on-hbase-using-sparksql" aria-hidden="true"><span class="octicon octicon-link"></span></a>Astro: Fast SQL on HBase using SparkSQL</h2>

<p>Astro is fully distributed SQL engine on HBase by leveraging Spark ecosystem. It enables systematic and powerful handling of data pruning, intelligent scan, and pushdowns like custom filters and coprocessor, and make more traditional RDBMS capabilities possible.
<img src="http://ravipesala.github.io/astro/images/astro_architecture.png" alt="ASTRO"></p>

<h2>
<a id="why-astro" class="anchor" href="#why-astro" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why Astro</h2>

<p>HBase is a very useful big data store but its access mechanism is very primitive and only through client-side APIs, Map/Reduce interfaces and interactive shells. Astro provides SQL layer over Hbase, so it takes your SQL query, compiles it into optimized Spark plan and in turn it does series of HBase scans to result sets.User do not require to write any complex client code to access HBase tables.
For example,to count rows in Hbase table we need to write following code:</p>

<pre><code>public class MyAggregationClient {
    private static final byte[] TABLE_NAME = Bytes.toBytes("CNCC_Demo");
    private static final byte[] CF = Bytes.toBytes("f1");
    public static void main(String[] args) throws Throwable 
    {
        Configuration customConf = new Configuration();
        customConf.setStrings("hbase.zookeeper.quorum","node0,node1,node2");
        customConf.setLong("hbase.rpc.timeout", 600000);
        customConf.setLong("hbase.client.scanner.caching", 1000);
        Configuration configuration = HBaseConfiguration.create(customConf);
        AggregationClient aggregationClient = new AggregationClient(configuration);
        Scan scan = new Scan();        
        scan.addFamily(CF);
        long rowCount = aggregationClient.rowCount(TABLE_NAME, null, scan);
        System.out.println("row count is " + rowCount);    
     }
}
</code></pre>

<p>But if we use Astro,create table schema and just one SQL query:</p>

<pre><code>CREATE TABLE CNCC_Demo (
      index          INTEGER,
      event_time     STRING,
      country        STRING,
      site           STRING,
      PRIMARY KEY(index))
      MAPPED BY (CNCC_Demo, COLS=   [event_time=f1.c1,country=f1.c2,site=f2.c1]);
select  count(*)  from CNCC_Demo;
</code></pre>

<h2>
<a id="quick-start" class="anchor" href="#quick-start" aria-hidden="true"><span class="octicon octicon-link"></span></a>Quick Start</h2>

<p>The easiest way to start using Astro is through the shell:</p>

<h3>
<a id="interactive-scala-shell" class="anchor" href="#interactive-scala-shell" aria-hidden="true"><span class="octicon octicon-link"></span></a>Interactive Scala Shell</h3>

<p>In this shell Spark context and as well as HBaseSQLContext is already created, launch the Astro shell as per the following method, and in this shell should be SQL apart from HELP and EXIT.</p>

<pre><code>&gt;./bin/hbase-sql
Welcome to hbaseql CLI

astro&gt;show tables;
OK
+---------+-----------+
|tableName|isTemporary|
+---------------------+
|Employee |      false|
+---------------------+

Time taken : 1.231 seconds
astro&gt;help;
Usage: HELP Statement
      Statement:
         CREATE | DROP | ALTER | LOAD | SELECT | INSERT | DESCRIBE | SHOW
</code></pre>

<h3>
<a id="python-shell" class="anchor" href="#python-shell" aria-hidden="true"><span class="octicon octicon-link"></span></a>Python Shell</h3>

<p>First, add the spark-hbase jar to the SPARK_CLASSPATH in the $SPARK_HOME/conf directory, as follows:</p>

<pre><code>SPARK_CLASSPATH=$SPARK_CLASSPATH:/spark-hbase-root-dir/target/spark-sql-on-hbase-1.0.0.jar
</code></pre>

<p>Then go to the Astro installation directory and launch the Astro shell:</p>

<pre><code>./bin/pyspark-hbase
</code></pre>

<p>A successful message is as follows:
You are using Spark SQL on HBase! HBaseSQLContext is available as hsqlContext.</p>

<p>To run a python script, the PYTHONPATH environment should be set to the "python" directory of the Spark-HBase installation. For example,</p>

<pre><code>export PYTHONPATH=/root-of-Spark-HBase/python
</code></pre>

<h2>
<a id="sql-support" class="anchor" href="#sql-support" aria-hidden="true"><span class="octicon octicon-link"></span></a>SQL Support</h2>

<p>Queries and data types will be the same as what SparkSQL supports. The differences will be in DDL and DML.</p>

<h3>
<a id="ddl" class="anchor" href="#ddl" aria-hidden="true"><span class="octicon octicon-link"></span></a>DDL</h3>

<p>Note that all DDL statements only affect the logical SQL table and not the physical tables.</p>

<h3>
<a id="create-table" class="anchor" href="#create-table" aria-hidden="true"><span class="octicon octicon-link"></span></a>CREATE TABLE</h3>

<p>A create table statement will be of the form of:</p>

<pre><code>CREATE TABLE table_name (col1 TYPE1, col2 TYPE2, …, PRIMARY KEY (col7, col1, col3)) 
MAPPED BY (hbase_tablename, COLS=[col2=cf1.cq11, col4=cf1.cq12, col5=cf2.cq21, col6=cf2.cq22])
</code></pre>

<p>A SQL table on HBASE is a basically logical table mapped to a HBase table,this mapping can be many-to-one to support “schema-on-read” for SQL access to HBase data.</p>

<ul>
<li>“hbase_table_name” denotes the HBase table</li>
<li>“primary key” constraint denotes the HBase row key composition of columns</li>
<li>“col2=cf1.cq1” denotes the mapping of the second column to the HBase tables column qualifier of “cq1” of column family “cf1”. Note : The table and the column families specified have to be exist in HBase for the CREATE TABLE statement to succeed. In addition, the columns in the primary key cannot be mapped to another column family/column qualifier combo. Other normal SQL sanity checks, such as uniqueness of logical columns, will be applied as well.</li>
<li>Row Key Composition :
HBase row keys will be composed in the way of big endian for processing efficiency. Keys or key components of the STRING type are marked with a NULL terminator.</li>
</ul>

<h3>
<a id="drop-table" class="anchor" href="#drop-table" aria-hidden="true"><span class="octicon octicon-link"></span></a>DROP TABLE</h3>

<p>A drop table statement is of the form of</p>

<pre><code>DROP TABLE table_name;
</code></pre>

<p>This will not delete the HBase table the SQL table maps to, but just deletes the SQL table with its schema.</p>

<h3>
<a id="alter-table" class="anchor" href="#alter-table" aria-hidden="true"><span class="octicon octicon-link"></span></a>ALTER TABLE</h3>

<pre><code>ALTER TABLE table_name DROP column;
</code></pre>

<p>Drops an existing column from the SQL table.</p>

<pre><code>ALTER TABLE table_name ADD col1 TYPE1 MAPPED BY (col1 = cf.cq)
</code></pre>

<p>Adds a new column that is mapped to existing column family “cf” and column qualifier “cq”,
ALTER TABLE does not support addition or deletion of components in the composite row key</p>

<h2>
<a id="dml" class="anchor" href="#dml" aria-hidden="true"><span class="octicon octicon-link"></span></a>DML</h2>

<h3>
<a id="insert" class="anchor" href="#insert" aria-hidden="true"><span class="octicon octicon-link"></span></a>INSERT</h3>

<p>The syntax remains the same as SchemaRDD’s. One constraint is that all columns in the HBASE key must be present for insertion to succeed. Normal SQL sanity checks for INSERT, such as uniqueness of logical columns, will be applied. There are two types of inserts. The first has the following syntax:</p>

<pre><code>INSERT INTO TABLE table_name  VALUES (col1_value, col2_value, …);
</code></pre>

<p>While the second has</p>

<pre><code>INSERT INTO TABLE table1_name SELECT … FROM table2_name;
</code></pre>

<h3>
<a id="bulk-loading" class="anchor" href="#bulk-loading" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bulk Loading</h3>

<pre><code>LOAD DATA [PARALLEL] INPATH filePath [OVERWRITE] INTO TABLE tableName [FIELDS TERMINATED BY char]
</code></pre>

<p>It is similar to Hive command, it is used to invoke a bulk loading into existing HBase tables. A “parallel” option will merge the “incremental loading” phase into the HFile generation phase. Conceivably it will perform better, particularly for non-presplit tables.</p>

<h3>
<a id="data-frame" class="anchor" href="#data-frame" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Frame</h3>

<p>Data Frame’s functionalities are supported in Astro. An example statement is as follows:</p>

<pre><code>val  hbaseContext =  new HBaseSQLContext(sc)
hbaseContext.read.format(“org.apache.spark.sql.hbase.HBaseSource”).options(
    Map("namespace" -&gt; "", "tableName" -&gt; "people", "hbaseTableName" -&gt; "people_table",
      "colsSeq" -&gt; "name,age,id,address",
      "keyCols" -&gt; "id,integer",
      "nonKeyCols" -&gt;   
        "name,string,cf1,cq_name;age,integer,cf1,cq_age;address,string,cf2,cq_address")).load
hbaseContext.sql("Select  `personal_data:name`, `personal_data:identification` as b, `personal_data
hbaseContext.sql("Select  name,  id, address  from people").collect.foreach(println)
</code></pre>

<p>There will be a few potentially subtle difference worth of caution. In particular, the methods of “registerAsTable” will not “register” a HBase-based table but an usual SparkSQL table. On the other hand, “insertInto” and “saveAsTable method will insert data into an existing SQL table on top of a HBase table.</p>

<h3>
<a id="metadata-persistence" class="anchor" href="#metadata-persistence" aria-hidden="true"><span class="octicon octicon-link"></span></a>Metadata Persistence</h3>

<p>Table meta data is stored in a HBase table named “SPARK_SQL_HBASE_TABLE”, of a single column family named “CF”. Each SQL table uses a single row in the HBase table. And each column will store the name and type encoding of a column of the SQL table.</p>

<h2>
<a id="fork-and-contribute" class="anchor" href="#fork-and-contribute" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fork and Contribute</h2>

<p>This is an 100% open source project, we are always open to people who want to use the system or contribute to it. 
This guide document introduce <a href="https://github.com/HuaweiBigData/astro/wiki/How-to-contribute-to-Astro">how to contribute to Astro</a>.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/HuaweiBigData/astro">Astro</a> is maintained by <a href="https://github.com/HuaweiBigData">HuaweiBigData</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
